{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import math\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Загрузка предобученной модели/токенизатора \n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloding dataset...\n",
      "dataset downloded and saved to cache\n",
      "Making tikenization...\n",
      "padded array saved to cache.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # try to load from preprocessed ndarray file\n",
    "    padded = np.load('datasets/padded.npy')\n",
    "    print('Padded array loaded from cache.')\n",
    "except IOError:\n",
    "    # loding and preprocessing raw data\n",
    "    local_data = 'datasets/toxic_comments.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(local_data)\n",
    "        print('dataset loaded from cache')\n",
    "    except IOError:\n",
    "        print('downloding dataset...')\n",
    "        df = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')\n",
    "        df = df.drop(df.columns[0], axis=1)\n",
    "        df.to_csv(local_data, index=False)\n",
    "        print('dataset downloded and saved to cache')\n",
    "        \n",
    "    # Сделаем токенизацию\n",
    "    print('Making tikenization...')\n",
    "    tokenized = df['text'].apply((lambda x: tokenizer.encode(x[:512], add_special_tokens=True)))\n",
    "    \n",
    "    # Найдём максимальную длину списка\n",
    "    max_len = len(max(tokenized, key=len))\n",
    "\n",
    "    # Приведем весь список к одинаковой длине\n",
    "    padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "    \n",
    "    # Сохраним массив в кеш\n",
    "    np.save('datasets/padded.npy', padded)\n",
    "    print('padded array saved to cache.')\n",
    "    \n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    \"\"\"Timer decorator\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = datetime.now()\n",
    "        result = func(*args, **kwargs)\n",
    "        total_time = (datetime.now() - start).total_seconds()\n",
    "        print(f'Total timelimit: {total_time}')\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timer\n",
    "def make_features(model, padded, attention_mask, batch_size: int, use_cache:bool = True, mmap_mode:bool = False) -> np.ndarray:\n",
    "    \"\"\"Function makes features from padded array\n",
    "\n",
    "    Args:\n",
    "        model (torch model): pretrined torch model\n",
    "        padded (ndarry): padded array\n",
    "        attention_mask (ndarray): attention mask\n",
    "        batch_size (int): batch size for prediction\n",
    "        use_cache (bool, optional): use cahce for save/load file. Defaults to True.\n",
    "        mmap_mode (bool, optional): Read or load features file from cache. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: features array\n",
    "    \"\"\"\n",
    "    if use_cache:\n",
    "        try:\n",
    "            # try to load embeddings from ndarray file\n",
    "            if mmap_mode:\n",
    "                features = np.load('datasets/features.npy', mmap_mode='r')\n",
    "            else:\n",
    "                features = np.load('datasets/features.npy')\n",
    "            print('Features loaded from cache.')\n",
    "            return features\n",
    "        except IOError:\n",
    "            pass      \n",
    "            \n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    if cuda_available:\n",
    "        # Select first CUDA device\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(\"Используется устройство:\", torch.cuda.get_device_name(device))\n",
    "\n",
    "        # move model to CUDA device\n",
    "        model = model.to(device)\n",
    "\n",
    "    embeddings = []\n",
    "    loop_range = range(math.ceil(padded.shape[0] / batch_size))\n",
    "    timer_arr = [] # array for calculate execution time\n",
    "    for i in loop_range:\n",
    "        timer = datetime.now()\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        \n",
    "        if cuda_available:\n",
    "            # move tensors to CUDA device\n",
    "            batch = batch.to(device)\n",
    "            attention_mask_batch = attention_mask_batch.to(device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "            \n",
    "        # .cpu() method needs if CUDA device used\n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())\n",
    "        \n",
    "        timer_arr.append((datetime.now() - timer).total_seconds())\n",
    "        \n",
    "        # print statistic\n",
    "        if i%100 == 0:\n",
    "            mean_iteration_time = np.array(timer_arr).mean()\n",
    "            timeleft = int((len(loop_range) - i) * mean_iteration_time)\n",
    "            print(f'{i}/{len(loop_range)} timeleft {timeleft} sec.')\n",
    "    \n",
    "    features = np.concatenate(embeddings)\n",
    "    \n",
    "    if use_cache:\n",
    "        # Сохраним массив в кеш\n",
    "        np.save('datasets/features.npy', features)\n",
    "        print('features array saved to cache.')\n",
    "    \n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется устройство: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "0/1245 timeleft 5336 sec.\n",
      "100/1245 timeleft 3939 sec.\n",
      "200/1245 timeleft 3598 sec.\n",
      "300/1245 timeleft 3259 sec.\n",
      "400/1245 timeleft 2916 sec.\n",
      "500/1245 timeleft 2572 sec.\n",
      "600/1245 timeleft 2229 sec.\n",
      "700/1245 timeleft 1884 sec.\n",
      "800/1245 timeleft 1539 sec.\n",
      "900/1245 timeleft 1193 sec.\n",
      "1000/1245 timeleft 847 sec.\n",
      "1100/1245 timeleft 501 sec.\n",
      "1200/1245 timeleft 155 sec.\n",
      "features array saved to cache.\n",
      "Total timelimit: 4311.726253\n"
     ]
    }
   ],
   "source": [
    "features = make_features(model, padded, attention_mask, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, df['toxic'], test_size=0.25)\n",
    "\n",
    "# обучите и протестируйте модель\n",
    "lr_model = LogisticRegression(max_iter=2000)\n",
    "lr_model.fit(x_train, y_train)\n",
    "pred = lr_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7298747763864043"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
