{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data, load_data_from_list\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_env\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_observation_window\n",
      "File \u001b[0;32m~/code/ml/trading/trainer/policy.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_policy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_custom_policy\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'"
     ]
    }
   ],
   "source": [
    "from trainer.data import load_data, load_data_from_list\n",
    "from trainer.policy import get_model\n",
    "from trainer.env import make_env\n",
    "from trainer.preprocessing import make_observation_window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from trade_tester.tester_base_class import render_candles\n",
    "import xarray as xr\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "# отключим GPU (нужно для платформы Apple M1)\n",
    "# tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_kwargs = dict(\n",
    "    path = 'klines/',\n",
    "    symbol = 'DOGEUSDT',\n",
    "    tf='15m',\n",
    "    preprocessing_kwargs = dict(\n",
    "        bb = dict(period=20, render=True, deviation=1.8),\n",
    "        rsi = dict(period=14, render=True, separately=True, color='blue'),\n",
    "        # ma = dict(period=20, render=True, color='red'),\n",
    "        obv = dict(render=True, color='green', separately=True),\n",
    "    ),\n",
    "    split_validate_percent = 0,\n",
    ")\n",
    "train_klines, val_klines, indicators, dataset = load_data(**load_data_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_klines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = dict(\n",
    "    env_class='TradingEnv2BoxAction',\n",
    "    tester='BBTester',\n",
    "    klines=train_klines,\n",
    "    data=dataset,\n",
    "    indicators=indicators,\n",
    "    b_size=1000,\n",
    ")\n",
    "env = make_env(**env_kwargs)\n",
    "# model = get_model(env=env)\n",
    "model = PPO('MlpPolicy', env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done=False\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step([action])\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('features_extractor',\n",
       "              FlattenExtractor(\n",
       "                (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "              )),\n",
       "             ('pi_features_extractor',\n",
       "              FlattenExtractor(\n",
       "                (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "              )),\n",
       "             ('vf_features_extractor',\n",
       "              FlattenExtractor(\n",
       "                (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "              )),\n",
       "             ('mlp_extractor',\n",
       "              MlpExtractor(\n",
       "                (shared_net): Sequential()\n",
       "                (policy_net): Sequential(\n",
       "                  (0): Linear(in_features=1, out_features=64, bias=True)\n",
       "                  (1): Tanh()\n",
       "                  (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (3): Tanh()\n",
       "                )\n",
       "                (value_net): Sequential(\n",
       "                  (0): Linear(in_features=1, out_features=64, bias=True)\n",
       "                  (1): Tanh()\n",
       "                  (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (3): Tanh()\n",
       "                )\n",
       "              )),\n",
       "             ('action_net', Linear(in_features=64, out_features=2, bias=True)),\n",
       "             ('value_net', Linear(in_features=64, out_features=1, bias=True))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлеките параметры политики\n",
    "policy_parameters = model.policy.state_dict()\n",
    "# Выведите веса каждого слоя\n",
    "for name, weights in policy_parameters.items():\n",
    "    print(f\"{name}: {weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Исследование структуры model.policy\n",
    "def explore_structure(obj, indent=0):\n",
    "    for name in dir(obj):\n",
    "        if not name.startswith(\"__\"):\n",
    "            value = getattr(obj, name)\n",
    "            if not callable(value):\n",
    "                print(\"  \" * indent, f\"{name}: {type(value)}\")\n",
    "                if isinstance(value, torch.nn.Module):\n",
    "                    explore_structure(value, indent + 1)\n",
    "\n",
    "# print(\"Policy structure:\")\n",
    "# explore_structure(model.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "obs = th.tensor(obs)\n",
    "obs = obs.transpose(1, 2)[0]\n",
    "for channel in range(obs.shape[0]):\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.nn import CustomNN\n",
    "\n",
    "nn = CustomNN(np.array([[9, 300], [9, 300]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in nn.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    symbols = ['DOGEUSDT', 'DOGEBTC', 'BTCUSDT'],\n",
    "    tfs = ['15m', '30m', '1h', '4h'],\n",
    "    preprocessing_kwargs = dict(\n",
    "                    bb = dict(period=20, deviation=2),\n",
    "                    rsi = dict(period=14),\n",
    "                    ma = dict(period=20),\n",
    "                    obv = dict(),\n",
    "                ),\n",
    ")\n",
    "dfs = load_data_from_list(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find min and max date over dfs\n",
    "min_dates = []\n",
    "for df in dfs:\n",
    "    min_dates.append(df['date'].min())\n",
    "min_date = max(min_dates)\n",
    "\n",
    "# shrink dataset so that the minimum date is the same\n",
    "dfs2 = []\n",
    "for df in dfs:\n",
    "    dfs2.append(df[df['date'] >= min_date].reset_index(drop=True))\n",
    "dfs = dfs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "window = 100\n",
    "length = len(dfs[0]) - window\n",
    "for i in range(1600+window+1, window+length):\n",
    "    a = make_observation_window(dfs, date=dfs[0].iloc[i][0], window=window)\n",
    "    dataset.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(dataset)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем xarray.DataArray с метками\n",
    "date = dfs[0][:dataset.shape[0]]['date'].values\n",
    "dataset = xr.DataArray(\n",
    "    dataset,\n",
    "    coords={'date': date},\n",
    "    dims=['date', 'n', 'chanel']\n",
    ")\n",
    "dataset.to_netcdf('dataset.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = xr.open_dataarray('dataset.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на обучающую и тестовую выборки\n",
    "split_index = int(0.8 * dataset.shape[0])\n",
    "train_data = dataset[:split_index].to_numpy()\n",
    "test_data = dataset[split_index:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1d_autoencoder(input_shape=(100, 144)):\n",
    "    # Encoder\n",
    "    encoder = Sequential([\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(pool_size=2, padding='same'),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')\n",
    "    ])\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = Sequential([\n",
    "        layers.Conv1D(32, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.UpSampling1D(size=2),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Conv1D(144, kernel_size=3, activation='sigmoid', padding='same')\n",
    "    ])\n",
    "    \n",
    "    # Autoencoder\n",
    "    autoencoder = Sequential([\n",
    "        encoder,\n",
    "        decoder\n",
    "    ])\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(decoded[0, :, :4], columns=['open', 'high', 'low', 'close']).reset_index()\n",
    "df = df.rename(columns={'index': 'date'})\n",
    "\n",
    "df2 = pd.DataFrame(test_data[0, :, :4], columns=['open', 'high', 'low', 'close']).reset_index()\n",
    "df2 = df2.rename(columns={'index': 'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_candles(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_candles(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(decoded[0].transpose(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(test_data[0].transpose(), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
