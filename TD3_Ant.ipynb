{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXu1r8qvSzWf"
   },
   "source": [
    "# Twin-Delayed DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRzQUhuUTc0J"
   },
   "source": [
    "## Installing the packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xjm2onHdT-Av"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ikr2p0Js8iB4"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'env_specs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpybullet_envs\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pybullet_envs/__init__.py:14\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m gym\u001b[39m.\u001b[39menvs\u001b[39m.\u001b[39mregistration\u001b[39m.\u001b[39mregister(\u001b[39mid\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkvargs)\n\u001b[1;32m     12\u001b[0m \u001b[39m# ------------bullet-------------\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m register(\n\u001b[1;32m     15\u001b[0m     \u001b[39mid\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mHumanoidDeepMimicBackflipBulletEnv-v1\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m     entry_point\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpybullet_envs.deep_mimic.gym_env:HumanoidDeepMimicBackflipBulletEnv\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m     max_episode_steps\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m     reward_threshold\u001b[39m=\u001b[39;49m\u001b[39m2000.0\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m register(\n\u001b[1;32m     22\u001b[0m     \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mHumanoidDeepMimicWalkBulletEnv-v1\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     23\u001b[0m     entry_point\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpybullet_envs.deep_mimic.gym_env:HumanoidDeepMimicWalkBulletEnv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m     max_episode_steps\u001b[39m=\u001b[39m\u001b[39m2000\u001b[39m,\n\u001b[1;32m     25\u001b[0m     reward_threshold\u001b[39m=\u001b[39m\u001b[39m2000.0\u001b[39m,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m register(\n\u001b[1;32m     29\u001b[0m     \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCartPoleBulletEnv-v1\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     30\u001b[0m     entry_point\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpybullet_envs.bullet:CartPoleBulletEnv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m     max_episode_steps\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,\n\u001b[1;32m     32\u001b[0m     reward_threshold\u001b[39m=\u001b[39m\u001b[39m190.0\u001b[39m,\n\u001b[1;32m     33\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pybullet_envs/__init__.py:6\u001b[0m, in \u001b[0;36mregister\u001b[0;34m(id, *args, **kvargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mregister\u001b[39m(\u001b[39mid\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkvargs):\n\u001b[0;32m----> 6\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m registry\u001b[39m.\u001b[39;49menv_specs:\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m      8\u001b[0m   \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'env_specs'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import wrappers\n",
    "from torch.autograd import Variable\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2nGdtlKVydr"
   },
   "source": [
    "## Step 1: We initialize the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u5rW0IDB8nTO"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, max_size=1e6):\n",
    "    self.storage = []\n",
    "    self.max_size = max_size\n",
    "    self.ptr = 0\n",
    "\n",
    "  def add(self, transition):\n",
    "    if len(self.storage) == self.max_size:\n",
    "      self.storage[int(self.ptr)] = transition\n",
    "      self.ptr = (self.ptr + 1) % self.max_size\n",
    "    else:\n",
    "      self.storage.append(transition)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "    for i in ind: \n",
    "      state, next_state, action, reward, done = self.storage[i]\n",
    "      batch_states.append(np.array(state, copy=False))\n",
    "      batch_next_states.append(np.array(next_state, copy=False))\n",
    "      batch_actions.append(np.array(action, copy=False))\n",
    "      batch_rewards.append(np.array(reward, copy=False))\n",
    "      batch_dones.append(np.array(done, copy=False))\n",
    "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jb7TTaHxWbQD"
   },
   "source": [
    "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CeRW4D79HL0"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer_1 = nn.Linear(state_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRDDce8FXef7"
   },
   "source": [
    "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCee7gwR9Jrs"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, 1)\n",
    "    # Defining the second Critic neural network\n",
    "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_5 = nn.Linear(400, 300)\n",
    "    self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "  def forward(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Forward-Propagation on the second Critic Neural Network\n",
    "    x2 = F.relu(self.layer_4(xu))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzIDuONodenW"
   },
   "source": [
    "## Steps 4 to 15: Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzd0H1xukdKe"
   },
   "outputs": [],
   "source": [
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "    return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "    for it in range(iterations):\n",
    "      \n",
    "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "      next_action = self.actor_target(next_state)\n",
    "      \n",
    "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "      \n",
    "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "      \n",
    "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "      \n",
    "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "      \n",
    "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "      \n",
    "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "      if it % policy_freq == 0:\n",
    "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "  # Making a save method to save a trained model\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ka-ZRtQvjBex"
   },
   "source": [
    "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qabqiYdp9wDM"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(np.array(obs))\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGuKmH_ijf7U"
   },
   "source": [
    "## We set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFj6wbAo97lk"
   },
   "outputs": [],
   "source": [
    "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
    "seed = 0 # Random seed number\n",
    "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
    "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
    "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 100 # Size of the batch\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.005 # Target network update rate\n",
    "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hjwf2HCol3XP"
   },
   "source": [
    "## We create a file name for the two saved models: the Actor and Critic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fyH8N5z-o3o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_AntBulletEnv-v0_0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kop-C96Aml8O"
   },
   "source": [
    "## We create a folder inside which will be saved the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Src07lvY-zXb"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./results\"):\n",
    "  os.makedirs(\"./results\")\n",
    "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
    "  os.makedirs(\"./pytorch_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qEAzOd47mv1Z"
   },
   "source": [
    "## We create the PyBullet environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyQXJUIs-6BV"
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YdPG4HXnNsh"
   },
   "source": [
    "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3RufYec_ADj"
   },
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWEgDAQxnbem"
   },
   "source": [
    "## We create the policy network (the Actor model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTVvG7F8_EWg"
   },
   "outputs": [],
   "source": [
    "policy = TD3(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZI60VN2Unklh"
   },
   "source": [
    "## We create the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sd-ZsdXR_LgV"
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYOpCyiDnw7s"
   },
   "source": [
    "## We define a list where all the evaluation results over 10 episodes are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhC_5XJ__Orp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 9.804960\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluations = [evaluate_policy(policy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xm-4b3p6rglE"
   },
   "source": [
    "## We create a new folder directory in which the final results (videos of the agent) will be populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTL9uMd0ru03"
   },
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "max_episode_steps = env._max_episode_steps\n",
    "save_env_vid = False\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31n5eb03p-Fm"
   },
   "source": [
    "## We initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vN5EvxK_QhT"
   },
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9gsjvtPqLgT"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_ouY4NH_Y0I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 1000 Episode Num: 1 Reward: 509.5321309727999\n",
      "Total Timesteps: 1199 Episode Num: 2 Reward: 85.24298546742065\n",
      "Total Timesteps: 2199 Episode Num: 3 Reward: 493.0625611426411\n",
      "Total Timesteps: 3199 Episode Num: 4 Reward: 502.0910572287305\n",
      "Total Timesteps: 3317 Episode Num: 5 Reward: 52.40597755264022\n",
      "Total Timesteps: 3641 Episode Num: 6 Reward: 147.45876645778915\n",
      "Total Timesteps: 4641 Episode Num: 7 Reward: 497.79776067039495\n",
      "Total Timesteps: 5641 Episode Num: 8 Reward: 469.5290782209818\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 82.447957\n",
      "---------------------------------------\n",
      "Total Timesteps: 6641 Episode Num: 9 Reward: 487.7244689291053\n",
      "Total Timesteps: 7641 Episode Num: 10 Reward: 399.1654542557474\n",
      "Total Timesteps: 8129 Episode Num: 11 Reward: 230.12172417418336\n",
      "Total Timesteps: 9129 Episode Num: 12 Reward: 478.18407400627314\n",
      "Total Timesteps: 9263 Episode Num: 13 Reward: 71.29054656375665\n",
      "Total Timesteps: 9750 Episode Num: 14 Reward: 254.6649010453736\n",
      "Total Timesteps: 10750 Episode Num: 15 Reward: 446.43876909636384\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 84.872193\n",
      "---------------------------------------\n",
      "Total Timesteps: 11750 Episode Num: 16 Reward: 91.51734513805003\n",
      "Total Timesteps: 12750 Episode Num: 17 Reward: 95.01302940191502\n",
      "Total Timesteps: 13750 Episode Num: 18 Reward: 90.89523129397163\n",
      "Total Timesteps: 14750 Episode Num: 19 Reward: 97.57866697059896\n",
      "Total Timesteps: 15750 Episode Num: 20 Reward: 196.3208017772864\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 76.076236\n",
      "---------------------------------------\n",
      "Total Timesteps: 16750 Episode Num: 21 Reward: 85.9811403350921\n",
      "Total Timesteps: 17750 Episode Num: 22 Reward: 83.03170398415357\n",
      "Total Timesteps: 18750 Episode Num: 23 Reward: 193.71537124233336\n",
      "Total Timesteps: 19750 Episode Num: 24 Reward: 178.39043948605257\n",
      "Total Timesteps: 20750 Episode Num: 25 Reward: 96.006390670247\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 217.445645\n",
      "---------------------------------------\n",
      "Total Timesteps: 21750 Episode Num: 26 Reward: 296.0351939551759\n",
      "Total Timesteps: 22750 Episode Num: 27 Reward: 155.10331368535287\n",
      "Total Timesteps: 23750 Episode Num: 28 Reward: 195.64528821376697\n",
      "Total Timesteps: 24750 Episode Num: 29 Reward: 297.5359971984806\n",
      "Total Timesteps: 25750 Episode Num: 30 Reward: 124.28919830316995\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 109.663232\n",
      "---------------------------------------\n",
      "Total Timesteps: 26750 Episode Num: 31 Reward: 101.0710010127944\n",
      "Total Timesteps: 27750 Episode Num: 32 Reward: 125.36460420688599\n",
      "Total Timesteps: 28750 Episode Num: 33 Reward: 121.80695203126373\n",
      "Total Timesteps: 29750 Episode Num: 34 Reward: 293.9513318002874\n",
      "Total Timesteps: 30750 Episode Num: 35 Reward: 226.48913817884684\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 279.560717\n",
      "---------------------------------------\n",
      "Total Timesteps: 31750 Episode Num: 36 Reward: 386.8883205953206\n",
      "Total Timesteps: 32750 Episode Num: 37 Reward: 388.8045638206319\n",
      "Total Timesteps: 33750 Episode Num: 38 Reward: 127.69878039041562\n",
      "Total Timesteps: 34750 Episode Num: 39 Reward: 234.59949057678273\n",
      "Total Timesteps: 35750 Episode Num: 40 Reward: 244.0094454923314\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 204.452029\n",
      "---------------------------------------\n",
      "Total Timesteps: 36750 Episode Num: 41 Reward: 192.9457446532101\n",
      "Total Timesteps: 37750 Episode Num: 42 Reward: 278.352339309323\n",
      "Total Timesteps: 38750 Episode Num: 43 Reward: 333.70926311333136\n",
      "Total Timesteps: 39750 Episode Num: 44 Reward: 115.79701244103748\n",
      "Total Timesteps: 40750 Episode Num: 45 Reward: 214.11664206513316\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 326.781456\n",
      "---------------------------------------\n",
      "Total Timesteps: 41750 Episode Num: 46 Reward: 377.37683218203506\n",
      "Total Timesteps: 42750 Episode Num: 47 Reward: 263.1134152253191\n",
      "Total Timesteps: 43750 Episode Num: 48 Reward: 413.37522344307877\n",
      "Total Timesteps: 44750 Episode Num: 49 Reward: 178.47024930535824\n",
      "Total Timesteps: 45750 Episode Num: 50 Reward: 430.5783458003899\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 387.633159\n",
      "---------------------------------------\n",
      "Total Timesteps: 46750 Episode Num: 51 Reward: 373.40600639819763\n",
      "Total Timesteps: 47750 Episode Num: 52 Reward: 311.10371781384066\n",
      "Total Timesteps: 48304 Episode Num: 53 Reward: 46.30176744223837\n",
      "Total Timesteps: 48340 Episode Num: 54 Reward: -10.048024483118425\n",
      "Total Timesteps: 48392 Episode Num: 55 Reward: -8.51365029040068\n",
      "Total Timesteps: 49392 Episode Num: 56 Reward: 299.23406780818743\n",
      "Total Timesteps: 50392 Episode Num: 57 Reward: 383.71882559144507\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 475.159350\n",
      "---------------------------------------\n",
      "Total Timesteps: 51392 Episode Num: 58 Reward: 340.22220304155655\n",
      "Total Timesteps: 52392 Episode Num: 59 Reward: 206.18885481862142\n",
      "Total Timesteps: 53392 Episode Num: 60 Reward: 356.93427550177495\n",
      "Total Timesteps: 54392 Episode Num: 61 Reward: 299.56259608123605\n",
      "Total Timesteps: 55392 Episode Num: 62 Reward: 524.4172972554915\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 518.987639\n",
      "---------------------------------------\n",
      "Total Timesteps: 56392 Episode Num: 63 Reward: 650.1499241900214\n",
      "Total Timesteps: 56451 Episode Num: 64 Reward: 7.4289070064012055\n",
      "Total Timesteps: 56471 Episode Num: 65 Reward: -0.940563180979165\n",
      "Total Timesteps: 56505 Episode Num: 66 Reward: 5.134619285120017\n",
      "Total Timesteps: 56525 Episode Num: 67 Reward: -1.9388586195083501\n",
      "Total Timesteps: 56574 Episode Num: 68 Reward: 17.25093086048498\n",
      "Total Timesteps: 56651 Episode Num: 69 Reward: 41.34226689846529\n",
      "Total Timesteps: 56714 Episode Num: 70 Reward: 8.6050669507747\n",
      "Total Timesteps: 56744 Episode Num: 71 Reward: 7.347070775539706\n",
      "Total Timesteps: 56782 Episode Num: 72 Reward: 11.162289542870951\n",
      "Total Timesteps: 56802 Episode Num: 73 Reward: -0.0005737900043967237\n",
      "Total Timesteps: 56822 Episode Num: 74 Reward: -1.718653220274422\n",
      "Total Timesteps: 56842 Episode Num: 75 Reward: -2.1796594040942536\n",
      "Total Timesteps: 56862 Episode Num: 76 Reward: -0.8321699358604044\n",
      "Total Timesteps: 56882 Episode Num: 77 Reward: -2.152795363457021\n",
      "Total Timesteps: 56902 Episode Num: 78 Reward: -2.0859184788577103\n",
      "Total Timesteps: 56922 Episode Num: 79 Reward: -1.881068299438716\n",
      "Total Timesteps: 56942 Episode Num: 80 Reward: -2.9658471876905623\n",
      "Total Timesteps: 56962 Episode Num: 81 Reward: -1.7359317104717062\n",
      "Total Timesteps: 56982 Episode Num: 82 Reward: -2.242154166671533\n",
      "Total Timesteps: 57002 Episode Num: 83 Reward: -1.6556539741903031\n",
      "Total Timesteps: 57022 Episode Num: 84 Reward: -0.8234803099756522\n",
      "Total Timesteps: 57042 Episode Num: 85 Reward: -1.8630294908522522\n",
      "Total Timesteps: 57062 Episode Num: 86 Reward: -1.0257760221671877\n",
      "Total Timesteps: 57082 Episode Num: 87 Reward: -1.4129205261938926\n",
      "Total Timesteps: 58082 Episode Num: 88 Reward: 575.7673294680027\n",
      "Total Timesteps: 59082 Episode Num: 89 Reward: 516.0889450075397\n",
      "Total Timesteps: 60082 Episode Num: 90 Reward: 456.97688145645\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 348.314199\n",
      "---------------------------------------\n",
      "Total Timesteps: 61082 Episode Num: 91 Reward: 688.439868255867\n",
      "Total Timesteps: 62082 Episode Num: 92 Reward: 410.0626270196245\n",
      "Total Timesteps: 63082 Episode Num: 93 Reward: 190.9589823236926\n",
      "Total Timesteps: 64082 Episode Num: 94 Reward: 292.1678152932524\n",
      "Total Timesteps: 65082 Episode Num: 95 Reward: 533.3335014144028\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 532.578515\n",
      "---------------------------------------\n",
      "Total Timesteps: 66082 Episode Num: 96 Reward: 438.5712595602832\n",
      "Total Timesteps: 67082 Episode Num: 97 Reward: 118.51669970687354\n",
      "Total Timesteps: 67184 Episode Num: 98 Reward: 61.0733628083452\n",
      "Total Timesteps: 68184 Episode Num: 99 Reward: 617.5002444661047\n",
      "Total Timesteps: 68204 Episode Num: 100 Reward: 2.6601585232394522\n",
      "Total Timesteps: 68224 Episode Num: 101 Reward: 1.6513048874214484\n",
      "Total Timesteps: 68245 Episode Num: 102 Reward: 3.6322946388533315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 68555 Episode Num: 103 Reward: 157.75458856818088\n",
      "Total Timesteps: 69555 Episode Num: 104 Reward: 354.69788582976946\n",
      "Total Timesteps: 70555 Episode Num: 105 Reward: 501.6264263362407\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 503.348283\n",
      "---------------------------------------\n",
      "Total Timesteps: 71555 Episode Num: 106 Reward: 578.2095296499167\n",
      "Total Timesteps: 71645 Episode Num: 107 Reward: 25.325948786471223\n",
      "Total Timesteps: 72645 Episode Num: 108 Reward: 594.3145453725447\n",
      "Total Timesteps: 73645 Episode Num: 109 Reward: 381.44435557359657\n",
      "Total Timesteps: 74002 Episode Num: 110 Reward: 90.54041753565552\n",
      "Total Timesteps: 75002 Episode Num: 111 Reward: 364.88264709355644\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 232.393811\n",
      "---------------------------------------\n",
      "Total Timesteps: 76002 Episode Num: 112 Reward: 236.15349513394784\n",
      "Total Timesteps: 77002 Episode Num: 113 Reward: 309.3502371190625\n",
      "Total Timesteps: 77127 Episode Num: 114 Reward: 34.85640295017425\n",
      "Total Timesteps: 78127 Episode Num: 115 Reward: 365.5760571727568\n",
      "Total Timesteps: 78812 Episode Num: 116 Reward: 405.0177325743989\n",
      "Total Timesteps: 79812 Episode Num: 117 Reward: 426.7032190659423\n",
      "Total Timesteps: 80812 Episode Num: 118 Reward: 380.9915577188921\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 242.588737\n",
      "---------------------------------------\n",
      "Total Timesteps: 81812 Episode Num: 119 Reward: 222.77564313306257\n",
      "Total Timesteps: 82812 Episode Num: 120 Reward: 383.9237836291097\n",
      "Total Timesteps: 83812 Episode Num: 121 Reward: 364.27148084363864\n",
      "Total Timesteps: 84812 Episode Num: 122 Reward: 469.91806747323847\n",
      "Total Timesteps: 85812 Episode Num: 123 Reward: 337.9125005030118\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 293.518498\n",
      "---------------------------------------\n",
      "Total Timesteps: 86812 Episode Num: 124 Reward: 208.9482652948053\n",
      "Total Timesteps: 87056 Episode Num: 125 Reward: 11.492756524396695\n",
      "Total Timesteps: 87327 Episode Num: 126 Reward: 111.3601561931159\n",
      "Total Timesteps: 88327 Episode Num: 127 Reward: 336.10819755135515\n",
      "Total Timesteps: 89327 Episode Num: 128 Reward: 206.38143679937545\n",
      "Total Timesteps: 89432 Episode Num: 129 Reward: 11.023256044155529\n",
      "Total Timesteps: 90432 Episode Num: 130 Reward: 228.98225044084086\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 235.435074\n",
      "---------------------------------------\n",
      "Total Timesteps: 91432 Episode Num: 131 Reward: 293.76085489942113\n",
      "Total Timesteps: 91571 Episode Num: 132 Reward: 30.10542966690129\n",
      "Total Timesteps: 92571 Episode Num: 133 Reward: 121.25207345829446\n",
      "Total Timesteps: 93571 Episode Num: 134 Reward: 109.11780468073799\n",
      "Total Timesteps: 94571 Episode Num: 135 Reward: 384.1607380882965\n",
      "Total Timesteps: 95571 Episode Num: 136 Reward: 197.94363066708235\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 312.473370\n",
      "---------------------------------------\n",
      "Total Timesteps: 96571 Episode Num: 137 Reward: 374.19903995355406\n",
      "Total Timesteps: 97571 Episode Num: 138 Reward: 332.7092513998072\n",
      "Total Timesteps: 98571 Episode Num: 139 Reward: 252.2810302511884\n",
      "Total Timesteps: 99571 Episode Num: 140 Reward: 395.27947528062435\n",
      "Total Timesteps: 100571 Episode Num: 141 Reward: 228.58807735529032\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 421.519381\n",
      "---------------------------------------\n",
      "Total Timesteps: 101571 Episode Num: 142 Reward: 467.7667848080732\n",
      "Total Timesteps: 101658 Episode Num: 143 Reward: 29.44018766470581\n",
      "Total Timesteps: 102658 Episode Num: 144 Reward: 344.78547810759443\n",
      "Total Timesteps: 103026 Episode Num: 145 Reward: 134.89051275323578\n",
      "Total Timesteps: 103459 Episode Num: 146 Reward: 108.81770268003487\n",
      "Total Timesteps: 104459 Episode Num: 147 Reward: 332.6024516898956\n",
      "Total Timesteps: 105459 Episode Num: 148 Reward: 588.6451335568895\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 452.110654\n",
      "---------------------------------------\n",
      "Total Timesteps: 106459 Episode Num: 149 Reward: 404.5792524161831\n",
      "Total Timesteps: 107459 Episode Num: 150 Reward: 355.2162075522978\n",
      "Total Timesteps: 108459 Episode Num: 151 Reward: 524.677360611094\n",
      "Total Timesteps: 108479 Episode Num: 152 Reward: 5.253297430613442\n",
      "Total Timesteps: 108499 Episode Num: 153 Reward: 6.5676456678813215\n",
      "Total Timesteps: 108519 Episode Num: 154 Reward: 6.470688930489279\n",
      "Total Timesteps: 108539 Episode Num: 155 Reward: 6.08757009157649\n",
      "Total Timesteps: 108559 Episode Num: 156 Reward: 5.452711247828262\n",
      "Total Timesteps: 108579 Episode Num: 157 Reward: 5.874293204683396\n",
      "Total Timesteps: 108599 Episode Num: 158 Reward: 5.857516737154507\n",
      "Total Timesteps: 108619 Episode Num: 159 Reward: 5.599422262045094\n",
      "Total Timesteps: 108639 Episode Num: 160 Reward: 5.6307469151498575\n",
      "Total Timesteps: 108659 Episode Num: 161 Reward: 4.630273727894708\n",
      "Total Timesteps: 108679 Episode Num: 162 Reward: 5.498154799918227\n",
      "Total Timesteps: 108728 Episode Num: 163 Reward: 13.212823998590558\n",
      "Total Timesteps: 108750 Episode Num: 164 Reward: 2.3382373845415505\n",
      "Total Timesteps: 108770 Episode Num: 165 Reward: 3.64495380149936\n",
      "Total Timesteps: 108790 Episode Num: 166 Reward: 7.112379775931593\n",
      "Total Timesteps: 108810 Episode Num: 167 Reward: 7.098890459278066\n",
      "Total Timesteps: 108830 Episode Num: 168 Reward: 7.043092483293682\n",
      "Total Timesteps: 108850 Episode Num: 169 Reward: 7.191409486145194\n",
      "Total Timesteps: 108870 Episode Num: 170 Reward: 6.311463030495123\n",
      "Total Timesteps: 108890 Episode Num: 171 Reward: 6.504063657865881\n",
      "Total Timesteps: 108910 Episode Num: 172 Reward: 6.79936067147398\n",
      "Total Timesteps: 108930 Episode Num: 173 Reward: 6.961851250681574\n",
      "Total Timesteps: 108950 Episode Num: 174 Reward: 6.998580621232273\n",
      "Total Timesteps: 108970 Episode Num: 175 Reward: 7.807108169035941\n",
      "Total Timesteps: 108992 Episode Num: 176 Reward: 4.045216112664341\n",
      "Total Timesteps: 109012 Episode Num: 177 Reward: 3.3081027139054364\n",
      "Total Timesteps: 109032 Episode Num: 178 Reward: 6.721407536029464\n",
      "Total Timesteps: 109052 Episode Num: 179 Reward: 5.930141386359685\n",
      "Total Timesteps: 110052 Episode Num: 180 Reward: 337.0611895057965\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 353.876597\n",
      "---------------------------------------\n",
      "Total Timesteps: 111052 Episode Num: 181 Reward: 337.3863054683471\n",
      "Total Timesteps: 112052 Episode Num: 182 Reward: 663.2345302533425\n",
      "Total Timesteps: 113052 Episode Num: 183 Reward: 364.04139043424925\n",
      "Total Timesteps: 114052 Episode Num: 184 Reward: 168.47961809833254\n",
      "Total Timesteps: 115052 Episode Num: 185 Reward: 650.2676212387679\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 467.302266\n",
      "---------------------------------------\n",
      "Total Timesteps: 116052 Episode Num: 186 Reward: 541.8951275847523\n",
      "Total Timesteps: 117052 Episode Num: 187 Reward: 660.8019828888282\n",
      "Total Timesteps: 118052 Episode Num: 188 Reward: 233.7620938340067\n",
      "Total Timesteps: 119052 Episode Num: 189 Reward: 228.5881169658066\n",
      "Total Timesteps: 120052 Episode Num: 190 Reward: 419.738072477752\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 364.810065\n",
      "---------------------------------------\n",
      "Total Timesteps: 121052 Episode Num: 191 Reward: 280.38960833644774\n",
      "Total Timesteps: 122052 Episode Num: 192 Reward: 349.27147323563577\n",
      "Total Timesteps: 123052 Episode Num: 193 Reward: 209.8492430944854\n",
      "Total Timesteps: 124052 Episode Num: 194 Reward: 324.01793561254004\n",
      "Total Timesteps: 125052 Episode Num: 195 Reward: 598.7118600680017\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 400.476111\n",
      "---------------------------------------\n",
      "Total Timesteps: 126052 Episode Num: 196 Reward: 231.38379713084106\n",
      "Total Timesteps: 127052 Episode Num: 197 Reward: 479.84392875120307\n",
      "Total Timesteps: 128052 Episode Num: 198 Reward: 200.39364836960908\n",
      "Total Timesteps: 128246 Episode Num: 199 Reward: 83.50917369119264\n",
      "Total Timesteps: 129246 Episode Num: 200 Reward: 348.6779220672714\n",
      "Total Timesteps: 129325 Episode Num: 201 Reward: 11.589230245957292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 129382 Episode Num: 202 Reward: 7.2837200239002895\n",
      "Total Timesteps: 129437 Episode Num: 203 Reward: 2.7341678057668704\n",
      "Total Timesteps: 129605 Episode Num: 204 Reward: 43.878708634650074\n",
      "Total Timesteps: 130605 Episode Num: 205 Reward: 233.14881758611594\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 238.128429\n",
      "---------------------------------------\n",
      "Total Timesteps: 130834 Episode Num: 206 Reward: 105.68745438755785\n",
      "Total Timesteps: 131834 Episode Num: 207 Reward: 185.94747434562004\n",
      "Total Timesteps: 132834 Episode Num: 208 Reward: 190.10898129974024\n",
      "Total Timesteps: 133834 Episode Num: 209 Reward: 183.51426253106717\n",
      "Total Timesteps: 134834 Episode Num: 210 Reward: 115.72289650329743\n",
      "Total Timesteps: 135834 Episode Num: 211 Reward: 105.65260791495044\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 269.300287\n",
      "---------------------------------------\n",
      "Total Timesteps: 136834 Episode Num: 212 Reward: 299.21113941376797\n",
      "Total Timesteps: 137834 Episode Num: 213 Reward: 656.6628472700971\n",
      "Total Timesteps: 138834 Episode Num: 214 Reward: 185.04213845429973\n",
      "Total Timesteps: 139834 Episode Num: 215 Reward: 396.934300229501\n",
      "Total Timesteps: 140834 Episode Num: 216 Reward: 499.4254555685806\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 270.081925\n",
      "---------------------------------------\n",
      "Total Timesteps: 141834 Episode Num: 217 Reward: 372.0174003296706\n",
      "Total Timesteps: 142834 Episode Num: 218 Reward: 385.0000893545164\n",
      "Total Timesteps: 143344 Episode Num: 219 Reward: 142.25274447072545\n",
      "Total Timesteps: 144344 Episode Num: 220 Reward: 357.7153642061669\n",
      "Total Timesteps: 145344 Episode Num: 221 Reward: 287.7848038241852\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 360.022898\n",
      "---------------------------------------\n",
      "Total Timesteps: 145723 Episode Num: 222 Reward: 76.4628103497451\n",
      "Total Timesteps: 146723 Episode Num: 223 Reward: 365.3869488969434\n",
      "Total Timesteps: 147723 Episode Num: 224 Reward: 251.30786871572565\n",
      "Total Timesteps: 148723 Episode Num: 225 Reward: 346.3715187464116\n",
      "Total Timesteps: 149723 Episode Num: 226 Reward: 467.6039171984494\n",
      "Total Timesteps: 150723 Episode Num: 227 Reward: 435.3931965948286\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 470.636306\n",
      "---------------------------------------\n",
      "Total Timesteps: 151201 Episode Num: 228 Reward: 230.7380496793957\n",
      "Total Timesteps: 152201 Episode Num: 229 Reward: 226.63406955498576\n",
      "Total Timesteps: 153201 Episode Num: 230 Reward: 553.1557250339202\n",
      "Total Timesteps: 154201 Episode Num: 231 Reward: 332.85961800218115\n",
      "Total Timesteps: 155201 Episode Num: 232 Reward: 546.8447971263906\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 336.490666\n",
      "---------------------------------------\n",
      "Total Timesteps: 156201 Episode Num: 233 Reward: 353.76891404440033\n",
      "Total Timesteps: 157201 Episode Num: 234 Reward: 266.4584723688723\n",
      "Total Timesteps: 158201 Episode Num: 235 Reward: 332.6194019937227\n",
      "Total Timesteps: 159201 Episode Num: 236 Reward: 383.0037493628185\n",
      "Total Timesteps: 160201 Episode Num: 237 Reward: 588.2992121065955\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 622.873601\n",
      "---------------------------------------\n",
      "Total Timesteps: 161201 Episode Num: 238 Reward: 687.1745412184044\n",
      "Total Timesteps: 162201 Episode Num: 239 Reward: 494.4701094259624\n",
      "Total Timesteps: 163201 Episode Num: 240 Reward: 711.8389320877501\n",
      "Total Timesteps: 164201 Episode Num: 241 Reward: 337.09246719633745\n",
      "Total Timesteps: 165201 Episode Num: 242 Reward: 645.7913427412914\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 706.653986\n",
      "---------------------------------------\n",
      "Total Timesteps: 166201 Episode Num: 243 Reward: 706.0814012109838\n",
      "Total Timesteps: 167201 Episode Num: 244 Reward: 600.822354609447\n",
      "Total Timesteps: 168201 Episode Num: 245 Reward: 573.8876860427295\n",
      "Total Timesteps: 169201 Episode Num: 246 Reward: 701.863118297438\n",
      "Total Timesteps: 170201 Episode Num: 247 Reward: 678.1337341070596\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 508.716792\n",
      "---------------------------------------\n",
      "Total Timesteps: 171201 Episode Num: 248 Reward: 660.6322049282138\n",
      "Total Timesteps: 172201 Episode Num: 249 Reward: 666.1625669344542\n",
      "Total Timesteps: 173201 Episode Num: 250 Reward: 506.30380243732014\n",
      "Total Timesteps: 174201 Episode Num: 251 Reward: 640.395601796803\n",
      "Total Timesteps: 175201 Episode Num: 252 Reward: 541.2712775771963\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 582.683819\n",
      "---------------------------------------\n",
      "Total Timesteps: 176201 Episode Num: 253 Reward: 573.0675992669455\n",
      "Total Timesteps: 176616 Episode Num: 254 Reward: 143.1586601090395\n",
      "Total Timesteps: 177616 Episode Num: 255 Reward: 447.8392105935745\n",
      "Total Timesteps: 178616 Episode Num: 256 Reward: 630.2709944964723\n",
      "Total Timesteps: 179616 Episode Num: 257 Reward: 727.5775466475953\n",
      "Total Timesteps: 180616 Episode Num: 258 Reward: 646.0744891023215\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 494.910585\n",
      "---------------------------------------\n",
      "Total Timesteps: 181616 Episode Num: 259 Reward: 595.0309039728619\n",
      "Total Timesteps: 182616 Episode Num: 260 Reward: 516.962059196661\n",
      "Total Timesteps: 183616 Episode Num: 261 Reward: 313.40351379079107\n",
      "Total Timesteps: 184616 Episode Num: 262 Reward: 548.9685677434709\n",
      "Total Timesteps: 185616 Episode Num: 263 Reward: 423.79655053057775\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 390.716501\n",
      "---------------------------------------\n",
      "Total Timesteps: 186616 Episode Num: 264 Reward: 464.8448367937899\n",
      "Total Timesteps: 187616 Episode Num: 265 Reward: 606.3881801822323\n",
      "Total Timesteps: 188616 Episode Num: 266 Reward: 444.40281751305434\n",
      "Total Timesteps: 189616 Episode Num: 267 Reward: 326.5072570743322\n",
      "Total Timesteps: 190280 Episode Num: 268 Reward: 266.0994992398499\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 222.250793\n",
      "---------------------------------------\n",
      "Total Timesteps: 191120 Episode Num: 269 Reward: 428.2605741275236\n",
      "Total Timesteps: 192120 Episode Num: 270 Reward: 420.37651478645034\n",
      "Total Timesteps: 193120 Episode Num: 271 Reward: 516.8136765460889\n",
      "Total Timesteps: 193587 Episode Num: 272 Reward: 219.8546712123007\n",
      "Total Timesteps: 194587 Episode Num: 273 Reward: 479.785363254142\n",
      "Total Timesteps: 195587 Episode Num: 274 Reward: 328.95029814585826\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 71.860311\n",
      "---------------------------------------\n",
      "Total Timesteps: 195641 Episode Num: 275 Reward: 20.4527844982867\n",
      "Total Timesteps: 195967 Episode Num: 276 Reward: 127.80247183861273\n",
      "Total Timesteps: 196967 Episode Num: 277 Reward: 368.9061470788418\n",
      "Total Timesteps: 197967 Episode Num: 278 Reward: 527.7874161250962\n",
      "Total Timesteps: 197988 Episode Num: 279 Reward: 0.46519251918897275\n",
      "Total Timesteps: 198032 Episode Num: 280 Reward: 11.937740611986703\n",
      "Total Timesteps: 198053 Episode Num: 281 Reward: 0.11747696636333504\n",
      "Total Timesteps: 198329 Episode Num: 282 Reward: 146.0080727123166\n",
      "Total Timesteps: 198601 Episode Num: 283 Reward: 81.65452771535996\n",
      "Total Timesteps: 199601 Episode Num: 284 Reward: 569.0947325420431\n",
      "Total Timesteps: 200601 Episode Num: 285 Reward: 314.478271110486\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 331.836659\n",
      "---------------------------------------\n",
      "Total Timesteps: 201601 Episode Num: 286 Reward: 564.7985838303904\n",
      "Total Timesteps: 202601 Episode Num: 287 Reward: 306.92509027959665\n",
      "Total Timesteps: 203601 Episode Num: 288 Reward: 469.20835571011685\n",
      "Total Timesteps: 204601 Episode Num: 289 Reward: 550.155872869678\n",
      "Total Timesteps: 205601 Episode Num: 290 Reward: 484.270120624424\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 543.945648\n",
      "---------------------------------------\n",
      "Total Timesteps: 206601 Episode Num: 291 Reward: 549.42276074882\n",
      "Total Timesteps: 207601 Episode Num: 292 Reward: 888.2604995847827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 208601 Episode Num: 293 Reward: 509.9029035856506\n",
      "Total Timesteps: 209601 Episode Num: 294 Reward: 700.9473420803873\n",
      "Total Timesteps: 210601 Episode Num: 295 Reward: 610.1857881565464\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 452.440423\n",
      "---------------------------------------\n",
      "Total Timesteps: 211601 Episode Num: 296 Reward: 483.43079288573574\n",
      "Total Timesteps: 212601 Episode Num: 297 Reward: 604.3992826864272\n",
      "Total Timesteps: 213285 Episode Num: 298 Reward: 274.8922012141195\n",
      "Total Timesteps: 214125 Episode Num: 299 Reward: 288.45395179239523\n",
      "Total Timesteps: 215125 Episode Num: 300 Reward: 152.11375938176388\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 184.217653\n",
      "---------------------------------------\n",
      "Total Timesteps: 216125 Episode Num: 301 Reward: 252.74687401681072\n",
      "Total Timesteps: 216193 Episode Num: 302 Reward: 9.851973099935702\n",
      "Total Timesteps: 216213 Episode Num: 303 Reward: 0.5734312742148489\n",
      "Total Timesteps: 216247 Episode Num: 304 Reward: 4.280472710019178\n",
      "Total Timesteps: 216268 Episode Num: 305 Reward: 0.9462654335221798\n",
      "Total Timesteps: 216288 Episode Num: 306 Reward: 1.2921681048883555\n",
      "Total Timesteps: 216309 Episode Num: 307 Reward: 1.1477490245076414\n",
      "Total Timesteps: 216330 Episode Num: 308 Reward: 1.4583644553683803\n",
      "Total Timesteps: 216352 Episode Num: 309 Reward: 2.946935558809935\n",
      "Total Timesteps: 216435 Episode Num: 310 Reward: 17.623050365634445\n",
      "Total Timesteps: 216520 Episode Num: 311 Reward: 14.40777902821197\n",
      "Total Timesteps: 217520 Episode Num: 312 Reward: 205.23827387479363\n",
      "Total Timesteps: 218520 Episode Num: 313 Reward: 453.9693682287187\n",
      "Total Timesteps: 219520 Episode Num: 314 Reward: 342.5161459034537\n",
      "Total Timesteps: 220520 Episode Num: 315 Reward: 536.0862599866324\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 608.900039\n",
      "---------------------------------------\n",
      "Total Timesteps: 221520 Episode Num: 316 Reward: 493.3262935764526\n",
      "Total Timesteps: 222520 Episode Num: 317 Reward: 483.1978272498537\n",
      "Total Timesteps: 223520 Episode Num: 318 Reward: 315.28120651837\n",
      "Total Timesteps: 224520 Episode Num: 319 Reward: 509.63939382014877\n",
      "Total Timesteps: 225412 Episode Num: 320 Reward: 379.7883550186216\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 512.312715\n",
      "---------------------------------------\n",
      "Total Timesteps: 226412 Episode Num: 321 Reward: 584.1460454540569\n",
      "Total Timesteps: 226546 Episode Num: 322 Reward: 46.361003260762054\n",
      "Total Timesteps: 226566 Episode Num: 323 Reward: 0.5536835312955053\n",
      "Total Timesteps: 226586 Episode Num: 324 Reward: -0.42590793834878493\n",
      "Total Timesteps: 226606 Episode Num: 325 Reward: -0.4604265122389155\n",
      "Total Timesteps: 226626 Episode Num: 326 Reward: 0.7099457121763111\n",
      "Total Timesteps: 226647 Episode Num: 327 Reward: 0.7006894380866671\n",
      "Total Timesteps: 226669 Episode Num: 328 Reward: -0.11647906119357421\n",
      "Total Timesteps: 226690 Episode Num: 329 Reward: 0.35489559129373105\n",
      "Total Timesteps: 226711 Episode Num: 330 Reward: 0.34838630508665624\n",
      "Total Timesteps: 226732 Episode Num: 331 Reward: 0.6524922499979162\n",
      "Total Timesteps: 226752 Episode Num: 332 Reward: 0.30441002873920087\n",
      "Total Timesteps: 226772 Episode Num: 333 Reward: 0.6729099568555785\n",
      "Total Timesteps: 226793 Episode Num: 334 Reward: 1.6314350897799725\n",
      "Total Timesteps: 226816 Episode Num: 335 Reward: 4.500992448264242\n",
      "Total Timesteps: 226837 Episode Num: 336 Reward: 3.750456041056363\n",
      "Total Timesteps: 226857 Episode Num: 337 Reward: 3.010616868462461\n",
      "Total Timesteps: 226877 Episode Num: 338 Reward: 2.0616714391298308\n",
      "Total Timesteps: 226897 Episode Num: 339 Reward: 2.2898774418639216\n",
      "Total Timesteps: 226917 Episode Num: 340 Reward: 1.402146066510502\n",
      "Total Timesteps: 226937 Episode Num: 341 Reward: 1.1090269779970279\n",
      "Total Timesteps: 226957 Episode Num: 342 Reward: 2.729717056865069\n",
      "Total Timesteps: 226977 Episode Num: 343 Reward: 0.9831927845853916\n",
      "Total Timesteps: 226997 Episode Num: 344 Reward: 1.9758370020589258\n",
      "Total Timesteps: 227017 Episode Num: 345 Reward: 1.9264654389049785\n",
      "Total Timesteps: 227037 Episode Num: 346 Reward: 2.7571931755197365\n",
      "Total Timesteps: 227057 Episode Num: 347 Reward: 1.8023506240666736\n",
      "Total Timesteps: 227077 Episode Num: 348 Reward: 1.4511272920102203\n",
      "Total Timesteps: 227097 Episode Num: 349 Reward: 0.7392727625026088\n",
      "Total Timesteps: 227117 Episode Num: 350 Reward: 1.5022333896538962\n",
      "Total Timesteps: 227137 Episode Num: 351 Reward: 1.3143525529353148\n",
      "Total Timesteps: 227157 Episode Num: 352 Reward: 1.893195818887067\n",
      "Total Timesteps: 227177 Episode Num: 353 Reward: 2.0534714803178753\n",
      "Total Timesteps: 227197 Episode Num: 354 Reward: 1.9492047686283556\n",
      "Total Timesteps: 227217 Episode Num: 355 Reward: 2.090942724837186\n",
      "Total Timesteps: 227237 Episode Num: 356 Reward: 2.4423084577535756\n",
      "Total Timesteps: 227257 Episode Num: 357 Reward: 2.9148828944518574\n",
      "Total Timesteps: 227277 Episode Num: 358 Reward: 2.6030661235278396\n",
      "Total Timesteps: 227297 Episode Num: 359 Reward: 3.0081908585662918\n",
      "Total Timesteps: 227317 Episode Num: 360 Reward: 2.5534343225325724\n",
      "Total Timesteps: 227337 Episode Num: 361 Reward: 3.0822239012436485\n",
      "Total Timesteps: 227357 Episode Num: 362 Reward: 3.560061339316883\n",
      "Total Timesteps: 227377 Episode Num: 363 Reward: 4.229307311249763\n",
      "Total Timesteps: 227397 Episode Num: 364 Reward: 3.328791772316001\n",
      "Total Timesteps: 227417 Episode Num: 365 Reward: 2.8613288263036583\n",
      "Total Timesteps: 227437 Episode Num: 366 Reward: 2.2379617811911463\n",
      "Total Timesteps: 227457 Episode Num: 367 Reward: 2.538760198810803\n",
      "Total Timesteps: 227477 Episode Num: 368 Reward: 2.273015368272141\n",
      "Total Timesteps: 227497 Episode Num: 369 Reward: 2.8283708622424255\n",
      "Total Timesteps: 227517 Episode Num: 370 Reward: 1.4595980379257756\n",
      "Total Timesteps: 227537 Episode Num: 371 Reward: 1.7711662628958624\n",
      "Total Timesteps: 227557 Episode Num: 372 Reward: 1.2828148740593717\n",
      "Total Timesteps: 227577 Episode Num: 373 Reward: 1.5186196238687963\n",
      "Total Timesteps: 227597 Episode Num: 374 Reward: 2.75749768343537\n",
      "Total Timesteps: 227617 Episode Num: 375 Reward: 2.8108218140849406\n",
      "Total Timesteps: 227637 Episode Num: 376 Reward: 1.6359684137445951\n",
      "Total Timesteps: 227657 Episode Num: 377 Reward: 2.0105387206777654\n",
      "Total Timesteps: 227677 Episode Num: 378 Reward: 0.6745966809235044\n",
      "Total Timesteps: 227697 Episode Num: 379 Reward: 2.093102324363678\n",
      "Total Timesteps: 227717 Episode Num: 380 Reward: 2.484100022756639\n",
      "Total Timesteps: 227737 Episode Num: 381 Reward: 2.2427121445264855\n",
      "Total Timesteps: 227757 Episode Num: 382 Reward: 2.2634131922071283\n",
      "Total Timesteps: 227777 Episode Num: 383 Reward: 1.8926051462984725\n",
      "Total Timesteps: 227797 Episode Num: 384 Reward: 3.851279397052007\n",
      "Total Timesteps: 227817 Episode Num: 385 Reward: 4.590735039889944\n",
      "Total Timesteps: 227837 Episode Num: 386 Reward: 4.4350176940865635\n",
      "Total Timesteps: 227858 Episode Num: 387 Reward: 5.437807618852203\n",
      "Total Timesteps: 227878 Episode Num: 388 Reward: 2.994040948467464\n",
      "Total Timesteps: 227898 Episode Num: 389 Reward: 3.349322040930508\n",
      "Total Timesteps: 227918 Episode Num: 390 Reward: 3.7336319992416502\n",
      "Total Timesteps: 227938 Episode Num: 391 Reward: 3.451717266200021\n",
      "Total Timesteps: 227958 Episode Num: 392 Reward: 4.382111646660601\n",
      "Total Timesteps: 227978 Episode Num: 393 Reward: 4.210458797910891\n",
      "Total Timesteps: 227999 Episode Num: 394 Reward: 3.4458300207453587\n",
      "Total Timesteps: 228502 Episode Num: 395 Reward: 257.4294633213669\n",
      "Total Timesteps: 229038 Episode Num: 396 Reward: 291.6166473083853\n",
      "Total Timesteps: 230038 Episode Num: 397 Reward: 641.8581237290987\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 445.479792\n",
      "---------------------------------------\n",
      "Total Timesteps: 231038 Episode Num: 398 Reward: 655.3185051109201\n",
      "Total Timesteps: 232016 Episode Num: 399 Reward: 508.4735953508053\n",
      "Total Timesteps: 233016 Episode Num: 400 Reward: 437.7746603385183\n",
      "Total Timesteps: 234016 Episode Num: 401 Reward: 545.1584385135964\n",
      "Total Timesteps: 235016 Episode Num: 402 Reward: 529.7012722236698\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 23.432907\n",
      "---------------------------------------\n",
      "Total Timesteps: 235039 Episode Num: 403 Reward: 0.8858530403940894\n",
      "Total Timesteps: 235060 Episode Num: 404 Reward: -0.6150875038015324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 235236 Episode Num: 405 Reward: 62.22347192429352\n",
      "Total Timesteps: 235257 Episode Num: 406 Reward: 0.4678804932511782\n",
      "Total Timesteps: 235278 Episode Num: 407 Reward: 1.0925675174091558\n",
      "Total Timesteps: 235350 Episode Num: 408 Reward: 23.173731030219024\n",
      "Total Timesteps: 235371 Episode Num: 409 Reward: -0.8694774921477182\n",
      "Total Timesteps: 235429 Episode Num: 410 Reward: 12.803730077679582\n",
      "Total Timesteps: 235449 Episode Num: 411 Reward: 1.650176198452964\n",
      "Total Timesteps: 235469 Episode Num: 412 Reward: 1.1159129792819993\n",
      "Total Timesteps: 235490 Episode Num: 413 Reward: 0.5302650715714472\n",
      "Total Timesteps: 235510 Episode Num: 414 Reward: 1.5162036292003345\n",
      "Total Timesteps: 235530 Episode Num: 415 Reward: 1.788553868881202\n",
      "Total Timesteps: 235550 Episode Num: 416 Reward: 1.598013177477335\n",
      "Total Timesteps: 235570 Episode Num: 417 Reward: 1.6479064852099738\n",
      "Total Timesteps: 235974 Episode Num: 418 Reward: 198.02823123356853\n",
      "Total Timesteps: 235994 Episode Num: 419 Reward: 1.3732387613028156\n",
      "Total Timesteps: 236014 Episode Num: 420 Reward: 2.423610469674909\n",
      "Total Timesteps: 236036 Episode Num: 421 Reward: 0.05531038412576983\n",
      "Total Timesteps: 236059 Episode Num: 422 Reward: 0.7696481868804996\n",
      "Total Timesteps: 236083 Episode Num: 423 Reward: 0.586275965847046\n",
      "Total Timesteps: 236105 Episode Num: 424 Reward: 1.4843348090768358\n",
      "Total Timesteps: 236127 Episode Num: 425 Reward: 0.7382243320460287\n",
      "Total Timesteps: 236148 Episode Num: 426 Reward: -0.7764549379497554\n",
      "Total Timesteps: 236169 Episode Num: 427 Reward: -1.3616908454747463\n",
      "Total Timesteps: 236190 Episode Num: 428 Reward: -0.16353642968045312\n",
      "Total Timesteps: 236211 Episode Num: 429 Reward: -2.502718636265868\n",
      "Total Timesteps: 236232 Episode Num: 430 Reward: 1.506314434146132\n",
      "Total Timesteps: 236253 Episode Num: 431 Reward: -1.2407513967643995\n",
      "Total Timesteps: 236274 Episode Num: 432 Reward: -0.9477660067126996\n",
      "Total Timesteps: 236295 Episode Num: 433 Reward: -1.0484638646125224\n",
      "Total Timesteps: 236316 Episode Num: 434 Reward: 0.7409010393986724\n",
      "Total Timesteps: 236336 Episode Num: 435 Reward: 2.2425901692418826\n",
      "Total Timesteps: 236356 Episode Num: 436 Reward: 1.3644481819151406\n",
      "Total Timesteps: 236377 Episode Num: 437 Reward: 0.9655530285094822\n",
      "Total Timesteps: 237377 Episode Num: 438 Reward: 419.8745031922906\n",
      "Total Timesteps: 237420 Episode Num: 439 Reward: -5.135580711813421\n",
      "Total Timesteps: 237571 Episode Num: 440 Reward: 56.147222607751736\n",
      "Total Timesteps: 238571 Episode Num: 441 Reward: 532.5108092282215\n",
      "Total Timesteps: 239571 Episode Num: 442 Reward: 641.9424589387377\n",
      "Total Timesteps: 240571 Episode Num: 443 Reward: 438.1373359623774\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 528.492003\n",
      "---------------------------------------\n",
      "Total Timesteps: 241571 Episode Num: 444 Reward: 580.3895942801906\n",
      "Total Timesteps: 242571 Episode Num: 445 Reward: 483.98207477438336\n",
      "Total Timesteps: 243571 Episode Num: 446 Reward: 286.4047214766844\n",
      "Total Timesteps: 244571 Episode Num: 447 Reward: 314.8506663075674\n",
      "Total Timesteps: 245571 Episode Num: 448 Reward: 243.84545080329474\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 439.870366\n",
      "---------------------------------------\n",
      "Total Timesteps: 246571 Episode Num: 449 Reward: 415.5125157452104\n",
      "Total Timesteps: 247571 Episode Num: 450 Reward: 605.886455255088\n",
      "Total Timesteps: 248571 Episode Num: 451 Reward: 441.0734743270937\n",
      "Total Timesteps: 249571 Episode Num: 452 Reward: 493.7953738866308\n",
      "Total Timesteps: 250571 Episode Num: 453 Reward: 528.3868186640118\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 480.385623\n",
      "---------------------------------------\n",
      "Total Timesteps: 251571 Episode Num: 454 Reward: 622.3094659046169\n",
      "Total Timesteps: 252571 Episode Num: 455 Reward: 756.5173421852812\n",
      "Total Timesteps: 253571 Episode Num: 456 Reward: 557.7129508680091\n",
      "Total Timesteps: 254571 Episode Num: 457 Reward: 530.9094375843789\n",
      "Total Timesteps: 255571 Episode Num: 458 Reward: 426.02683219539364\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 543.630995\n",
      "---------------------------------------\n",
      "Total Timesteps: 256571 Episode Num: 459 Reward: 694.3667182025015\n",
      "Total Timesteps: 257571 Episode Num: 460 Reward: 546.4139306175066\n",
      "Total Timesteps: 258571 Episode Num: 461 Reward: 655.9912303330763\n",
      "Total Timesteps: 259571 Episode Num: 462 Reward: 488.69608803901065\n",
      "Total Timesteps: 260571 Episode Num: 463 Reward: 765.2622520588378\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 474.908570\n",
      "---------------------------------------\n",
      "Total Timesteps: 261571 Episode Num: 464 Reward: 412.22379279171497\n",
      "Total Timesteps: 262571 Episode Num: 465 Reward: 527.3599168053674\n",
      "Total Timesteps: 263571 Episode Num: 466 Reward: 400.92416028697636\n",
      "Total Timesteps: 264571 Episode Num: 467 Reward: 673.0758741178682\n",
      "Total Timesteps: 265571 Episode Num: 468 Reward: 326.8545053322723\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 626.142051\n",
      "---------------------------------------\n",
      "Total Timesteps: 266571 Episode Num: 469 Reward: 532.5842849507488\n",
      "Total Timesteps: 267571 Episode Num: 470 Reward: 427.5248180377515\n",
      "Total Timesteps: 268571 Episode Num: 471 Reward: 726.2185607317276\n",
      "Total Timesteps: 269571 Episode Num: 472 Reward: 544.0390471967548\n",
      "Total Timesteps: 270571 Episode Num: 473 Reward: 453.6691437898504\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 712.410825\n",
      "---------------------------------------\n",
      "Total Timesteps: 271571 Episode Num: 474 Reward: 583.3907720743451\n",
      "Total Timesteps: 272571 Episode Num: 475 Reward: 731.5203858075306\n",
      "Total Timesteps: 273571 Episode Num: 476 Reward: 481.2984362434952\n",
      "Total Timesteps: 274571 Episode Num: 477 Reward: 614.2910078803669\n",
      "Total Timesteps: 275571 Episode Num: 478 Reward: 357.5101943840751\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 661.139146\n",
      "---------------------------------------\n",
      "Total Timesteps: 276571 Episode Num: 479 Reward: 703.3340838727232\n",
      "Total Timesteps: 277571 Episode Num: 480 Reward: 441.29208813147756\n",
      "Total Timesteps: 278571 Episode Num: 481 Reward: 314.53858141116217\n",
      "Total Timesteps: 279571 Episode Num: 482 Reward: 355.90317612399633\n",
      "Total Timesteps: 280571 Episode Num: 483 Reward: 415.11573555638324\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 335.803859\n",
      "---------------------------------------\n",
      "Total Timesteps: 281478 Episode Num: 484 Reward: 363.7748908428359\n",
      "Total Timesteps: 282478 Episode Num: 485 Reward: 447.6464827521766\n",
      "Total Timesteps: 283478 Episode Num: 486 Reward: 427.0099014745278\n",
      "Total Timesteps: 284478 Episode Num: 487 Reward: 430.6184023963085\n",
      "Total Timesteps: 285478 Episode Num: 488 Reward: 535.9595092201847\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 412.056725\n",
      "---------------------------------------\n",
      "Total Timesteps: 285924 Episode Num: 489 Reward: 203.03148793004186\n",
      "Total Timesteps: 286924 Episode Num: 490 Reward: 404.8470277962157\n",
      "Total Timesteps: 287253 Episode Num: 491 Reward: 120.09123592675964\n",
      "Total Timesteps: 288253 Episode Num: 492 Reward: 400.7393011628052\n",
      "Total Timesteps: 289253 Episode Num: 493 Reward: 364.76615570343597\n",
      "Total Timesteps: 290253 Episode Num: 494 Reward: 468.8618793267569\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 437.281936\n",
      "---------------------------------------\n",
      "Total Timesteps: 291253 Episode Num: 495 Reward: 359.00106627090327\n",
      "Total Timesteps: 292253 Episode Num: 496 Reward: 642.2817383806598\n",
      "Total Timesteps: 293253 Episode Num: 497 Reward: 446.3677921219076\n",
      "Total Timesteps: 293400 Episode Num: 498 Reward: 89.43235536921857\n",
      "Total Timesteps: 294400 Episode Num: 499 Reward: 441.3491701871778\n",
      "Total Timesteps: 295400 Episode Num: 500 Reward: 528.5163073906667\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 392.657560\n",
      "---------------------------------------\n",
      "Total Timesteps: 295794 Episode Num: 501 Reward: 208.73579030064775\n",
      "Total Timesteps: 296273 Episode Num: 502 Reward: 241.47251193786957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 297273 Episode Num: 503 Reward: 482.56872034944905\n",
      "Total Timesteps: 298273 Episode Num: 504 Reward: 609.4421022705993\n",
      "Total Timesteps: 299273 Episode Num: 505 Reward: 616.2204969170947\n",
      "Total Timesteps: 300273 Episode Num: 506 Reward: 592.7408264265482\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 651.777394\n",
      "---------------------------------------\n",
      "Total Timesteps: 301273 Episode Num: 507 Reward: 567.3146402522351\n",
      "Total Timesteps: 302273 Episode Num: 508 Reward: 636.2477469553422\n",
      "Total Timesteps: 303273 Episode Num: 509 Reward: 634.5091319451244\n",
      "Total Timesteps: 304273 Episode Num: 510 Reward: 639.0514453647168\n",
      "Total Timesteps: 305273 Episode Num: 511 Reward: 597.7204941101594\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 684.945859\n",
      "---------------------------------------\n",
      "Total Timesteps: 306273 Episode Num: 512 Reward: 655.0948166366516\n",
      "Total Timesteps: 307273 Episode Num: 513 Reward: 538.9525478681077\n",
      "Total Timesteps: 308273 Episode Num: 514 Reward: 677.4764642963061\n",
      "Total Timesteps: 309273 Episode Num: 515 Reward: 636.2788781077568\n",
      "Total Timesteps: 310273 Episode Num: 516 Reward: 530.1277768938182\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 478.029065\n",
      "---------------------------------------\n",
      "Total Timesteps: 311273 Episode Num: 517 Reward: 465.2773865038487\n",
      "Total Timesteps: 312273 Episode Num: 518 Reward: 634.3869599944832\n",
      "Total Timesteps: 313273 Episode Num: 519 Reward: 619.9389302094057\n",
      "Total Timesteps: 314273 Episode Num: 520 Reward: 563.3196172343803\n",
      "Total Timesteps: 315273 Episode Num: 521 Reward: 646.3189534280365\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 468.669388\n",
      "---------------------------------------\n",
      "Total Timesteps: 316273 Episode Num: 522 Reward: 515.0546786467364\n",
      "Total Timesteps: 317273 Episode Num: 523 Reward: 562.0620322037387\n",
      "Total Timesteps: 318273 Episode Num: 524 Reward: 430.6487611188952\n",
      "Total Timesteps: 319273 Episode Num: 525 Reward: 463.36777000371194\n",
      "Total Timesteps: 320273 Episode Num: 526 Reward: 356.1875775015034\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 516.770636\n",
      "---------------------------------------\n",
      "Total Timesteps: 321273 Episode Num: 527 Reward: 654.5723759901575\n",
      "Total Timesteps: 322273 Episode Num: 528 Reward: 691.3929563706786\n",
      "Total Timesteps: 323273 Episode Num: 529 Reward: 428.54315815929647\n",
      "Total Timesteps: 324273 Episode Num: 530 Reward: 689.9205894107339\n",
      "Total Timesteps: 325273 Episode Num: 531 Reward: 422.4236755073073\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 559.926751\n",
      "---------------------------------------\n",
      "Total Timesteps: 326273 Episode Num: 532 Reward: 572.3263395207932\n",
      "Total Timesteps: 327273 Episode Num: 533 Reward: 614.4315671322388\n",
      "Total Timesteps: 328273 Episode Num: 534 Reward: 586.8335510575795\n",
      "Total Timesteps: 329273 Episode Num: 535 Reward: 634.0315800017344\n",
      "Total Timesteps: 330273 Episode Num: 536 Reward: 536.2276965091103\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 574.031803\n",
      "---------------------------------------\n",
      "Total Timesteps: 331273 Episode Num: 537 Reward: 873.3490961595395\n",
      "Total Timesteps: 332273 Episode Num: 538 Reward: 625.081479104667\n",
      "Total Timesteps: 333273 Episode Num: 539 Reward: 767.5182410860425\n",
      "Total Timesteps: 334273 Episode Num: 540 Reward: 493.9828821815645\n",
      "Total Timesteps: 335024 Episode Num: 541 Reward: 390.13018766402473\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 574.044778\n",
      "---------------------------------------\n",
      "Total Timesteps: 336024 Episode Num: 542 Reward: 510.4382817960852\n",
      "Total Timesteps: 337024 Episode Num: 543 Reward: 819.132039356745\n",
      "Total Timesteps: 338024 Episode Num: 544 Reward: 756.5933714974742\n",
      "Total Timesteps: 339024 Episode Num: 545 Reward: 505.11917291766963\n",
      "Total Timesteps: 340024 Episode Num: 546 Reward: 752.0342833244704\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 683.175952\n",
      "---------------------------------------\n",
      "Total Timesteps: 341024 Episode Num: 547 Reward: 662.0501431589079\n",
      "Total Timesteps: 342024 Episode Num: 548 Reward: 746.1554274260293\n",
      "Total Timesteps: 343024 Episode Num: 549 Reward: 571.736490946746\n",
      "Total Timesteps: 344024 Episode Num: 550 Reward: 564.1110026446438\n",
      "Total Timesteps: 345024 Episode Num: 551 Reward: 633.9194936671639\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 691.500703\n",
      "---------------------------------------\n",
      "Total Timesteps: 346024 Episode Num: 552 Reward: 658.4419482281933\n",
      "Total Timesteps: 347024 Episode Num: 553 Reward: 683.7812594882095\n",
      "Total Timesteps: 348024 Episode Num: 554 Reward: 556.9635062596769\n",
      "Total Timesteps: 349024 Episode Num: 555 Reward: 738.0128227243471\n",
      "Total Timesteps: 350024 Episode Num: 556 Reward: 467.78782440296743\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 589.815895\n",
      "---------------------------------------\n",
      "Total Timesteps: 351024 Episode Num: 557 Reward: 559.376590971333\n",
      "Total Timesteps: 352024 Episode Num: 558 Reward: 722.541264102692\n",
      "Total Timesteps: 353024 Episode Num: 559 Reward: 588.8958118523003\n",
      "Total Timesteps: 354024 Episode Num: 560 Reward: 659.0739583459417\n",
      "Total Timesteps: 355024 Episode Num: 561 Reward: 752.3721864359356\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 646.435385\n",
      "---------------------------------------\n",
      "Total Timesteps: 356024 Episode Num: 562 Reward: 845.8375575311916\n",
      "Total Timesteps: 357024 Episode Num: 563 Reward: 654.9761071472303\n",
      "Total Timesteps: 358024 Episode Num: 564 Reward: 608.2412918901028\n",
      "Total Timesteps: 359024 Episode Num: 565 Reward: 565.2345380660654\n",
      "Total Timesteps: 360024 Episode Num: 566 Reward: 635.0519558323842\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 622.831918\n",
      "---------------------------------------\n",
      "Total Timesteps: 361024 Episode Num: 567 Reward: 770.2778543043705\n",
      "Total Timesteps: 362024 Episode Num: 568 Reward: 663.2363326685141\n",
      "Total Timesteps: 363024 Episode Num: 569 Reward: 800.863346072556\n",
      "Total Timesteps: 364024 Episode Num: 570 Reward: 613.7698098763029\n",
      "Total Timesteps: 365024 Episode Num: 571 Reward: 463.3780074477445\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 550.419886\n",
      "---------------------------------------\n",
      "Total Timesteps: 366024 Episode Num: 572 Reward: 611.859890512225\n",
      "Total Timesteps: 367024 Episode Num: 573 Reward: 480.4546103341106\n",
      "Total Timesteps: 368024 Episode Num: 574 Reward: 447.3758374036042\n",
      "Total Timesteps: 369024 Episode Num: 575 Reward: 669.6580929650904\n",
      "Total Timesteps: 370024 Episode Num: 576 Reward: 634.3740574735966\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 657.607963\n",
      "---------------------------------------\n",
      "Total Timesteps: 371024 Episode Num: 577 Reward: 725.6212926284386\n",
      "Total Timesteps: 372024 Episode Num: 578 Reward: 664.6595083573555\n",
      "Total Timesteps: 373024 Episode Num: 579 Reward: 760.0290575720977\n",
      "Total Timesteps: 374024 Episode Num: 580 Reward: 594.0101494587097\n",
      "Total Timesteps: 375024 Episode Num: 581 Reward: 816.0943103523033\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 511.217598\n",
      "---------------------------------------\n",
      "Total Timesteps: 376024 Episode Num: 582 Reward: 717.6960902960082\n",
      "Total Timesteps: 377024 Episode Num: 583 Reward: 526.0117161036258\n",
      "Total Timesteps: 378024 Episode Num: 584 Reward: 852.9132171326854\n",
      "Total Timesteps: 379024 Episode Num: 585 Reward: 812.5972333414231\n",
      "Total Timesteps: 380024 Episode Num: 586 Reward: 690.7614135699796\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 629.392318\n",
      "---------------------------------------\n",
      "Total Timesteps: 381024 Episode Num: 587 Reward: 478.1048280997857\n",
      "Total Timesteps: 382024 Episode Num: 588 Reward: 840.3162490132263\n",
      "Total Timesteps: 383024 Episode Num: 589 Reward: 709.3826579189679\n",
      "Total Timesteps: 384024 Episode Num: 590 Reward: 806.8885903366404\n",
      "Total Timesteps: 385024 Episode Num: 591 Reward: 820.031695828707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 615.575260\n",
      "---------------------------------------\n",
      "Total Timesteps: 386024 Episode Num: 592 Reward: 608.4745505597042\n",
      "Total Timesteps: 387024 Episode Num: 593 Reward: 803.3270700592203\n",
      "Total Timesteps: 388024 Episode Num: 594 Reward: 659.3472290687433\n",
      "Total Timesteps: 389024 Episode Num: 595 Reward: 601.2874367794643\n",
      "Total Timesteps: 390024 Episode Num: 596 Reward: 805.8059288501272\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 682.753330\n",
      "---------------------------------------\n",
      "Total Timesteps: 391024 Episode Num: 597 Reward: 636.5768716021275\n",
      "Total Timesteps: 392024 Episode Num: 598 Reward: 676.6658256915656\n",
      "Total Timesteps: 393024 Episode Num: 599 Reward: 785.4321874395157\n",
      "Total Timesteps: 394024 Episode Num: 600 Reward: 574.5934995439965\n",
      "Total Timesteps: 395024 Episode Num: 601 Reward: 699.5787110580774\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 709.234119\n",
      "---------------------------------------\n",
      "Total Timesteps: 396024 Episode Num: 602 Reward: 748.6375540287227\n",
      "Total Timesteps: 397024 Episode Num: 603 Reward: 776.4534182926254\n",
      "Total Timesteps: 398024 Episode Num: 604 Reward: 588.080295560642\n",
      "Total Timesteps: 399024 Episode Num: 605 Reward: 533.3086651186081\n",
      "Total Timesteps: 400024 Episode Num: 606 Reward: 848.8021051828671\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 682.226784\n",
      "---------------------------------------\n",
      "Total Timesteps: 401024 Episode Num: 607 Reward: 764.2095293445786\n",
      "Total Timesteps: 402024 Episode Num: 608 Reward: 574.1954017694517\n",
      "Total Timesteps: 403024 Episode Num: 609 Reward: 750.2113362734171\n",
      "Total Timesteps: 404024 Episode Num: 610 Reward: 690.6185187849429\n",
      "Total Timesteps: 405024 Episode Num: 611 Reward: 601.9603811086834\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 714.660912\n",
      "---------------------------------------\n",
      "Total Timesteps: 406024 Episode Num: 612 Reward: 683.7841573033551\n",
      "Total Timesteps: 407024 Episode Num: 613 Reward: 775.2284865210065\n",
      "Total Timesteps: 408024 Episode Num: 614 Reward: 618.5972906582421\n",
      "Total Timesteps: 409024 Episode Num: 615 Reward: 656.6717182971488\n",
      "Total Timesteps: 410024 Episode Num: 616 Reward: 729.6963503112407\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 713.039571\n",
      "---------------------------------------\n",
      "Total Timesteps: 411024 Episode Num: 617 Reward: 690.3533286841754\n",
      "Total Timesteps: 412024 Episode Num: 618 Reward: 672.5660877313634\n",
      "Total Timesteps: 413024 Episode Num: 619 Reward: 696.7762426808171\n",
      "Total Timesteps: 414024 Episode Num: 620 Reward: 809.6921260289516\n",
      "Total Timesteps: 415024 Episode Num: 621 Reward: 450.2109656403742\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 729.513895\n",
      "---------------------------------------\n",
      "Total Timesteps: 416024 Episode Num: 622 Reward: 839.7660590624578\n",
      "Total Timesteps: 417024 Episode Num: 623 Reward: 476.2128944745796\n",
      "Total Timesteps: 418024 Episode Num: 624 Reward: 668.7373613061723\n",
      "Total Timesteps: 419024 Episode Num: 625 Reward: 766.0687743369886\n",
      "Total Timesteps: 420024 Episode Num: 626 Reward: 777.8482412795202\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 710.378558\n",
      "---------------------------------------\n",
      "Total Timesteps: 421024 Episode Num: 627 Reward: 684.7233995379931\n",
      "Total Timesteps: 422024 Episode Num: 628 Reward: 697.0703493628505\n",
      "Total Timesteps: 423024 Episode Num: 629 Reward: 761.8104606567999\n",
      "Total Timesteps: 424024 Episode Num: 630 Reward: 757.1302059724342\n",
      "Total Timesteps: 425024 Episode Num: 631 Reward: 658.0911896334621\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 650.295735\n",
      "---------------------------------------\n",
      "Total Timesteps: 426024 Episode Num: 632 Reward: 474.4497194861609\n",
      "Total Timesteps: 427024 Episode Num: 633 Reward: 779.6022556082703\n",
      "Total Timesteps: 428024 Episode Num: 634 Reward: 851.2389949363319\n",
      "Total Timesteps: 429024 Episode Num: 635 Reward: 754.4011010019145\n",
      "Total Timesteps: 430024 Episode Num: 636 Reward: 589.5334996892582\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 637.516820\n",
      "---------------------------------------\n",
      "Total Timesteps: 431024 Episode Num: 637 Reward: 898.5138297743189\n",
      "Total Timesteps: 432024 Episode Num: 638 Reward: 703.7173192551078\n",
      "Total Timesteps: 433024 Episode Num: 639 Reward: 856.4545699469227\n",
      "Total Timesteps: 434024 Episode Num: 640 Reward: 934.4289963560831\n",
      "Total Timesteps: 435024 Episode Num: 641 Reward: 878.0454159476284\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 786.903583\n",
      "---------------------------------------\n",
      "Total Timesteps: 436024 Episode Num: 642 Reward: 720.2020595632792\n",
      "Total Timesteps: 437024 Episode Num: 643 Reward: 864.9964483748844\n",
      "Total Timesteps: 438024 Episode Num: 644 Reward: 499.3283471308983\n",
      "Total Timesteps: 439024 Episode Num: 645 Reward: 763.6998822585319\n",
      "Total Timesteps: 440024 Episode Num: 646 Reward: 808.3274605399935\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 706.006517\n",
      "---------------------------------------\n",
      "Total Timesteps: 441024 Episode Num: 647 Reward: 422.9274915938179\n",
      "Total Timesteps: 442024 Episode Num: 648 Reward: 757.2546762557181\n",
      "Total Timesteps: 443024 Episode Num: 649 Reward: 689.1553782114837\n",
      "Total Timesteps: 444024 Episode Num: 650 Reward: 512.0290787121542\n",
      "Total Timesteps: 445024 Episode Num: 651 Reward: 658.0778714646583\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 587.579616\n",
      "---------------------------------------\n",
      "Total Timesteps: 446024 Episode Num: 652 Reward: 731.5665572348934\n",
      "Total Timesteps: 447024 Episode Num: 653 Reward: 802.7081624938774\n",
      "Total Timesteps: 448024 Episode Num: 654 Reward: 606.6531959772842\n",
      "Total Timesteps: 449024 Episode Num: 655 Reward: 574.7307318577865\n",
      "Total Timesteps: 450024 Episode Num: 656 Reward: 797.1015801757386\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 760.739672\n",
      "---------------------------------------\n",
      "Total Timesteps: 451024 Episode Num: 657 Reward: 741.9352314985507\n",
      "Total Timesteps: 452024 Episode Num: 658 Reward: 785.307773549438\n",
      "Total Timesteps: 453024 Episode Num: 659 Reward: 769.4483999122214\n",
      "Total Timesteps: 454024 Episode Num: 660 Reward: 759.640319656928\n",
      "Total Timesteps: 455024 Episode Num: 661 Reward: 618.9887351729138\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 765.903559\n",
      "---------------------------------------\n",
      "Total Timesteps: 456024 Episode Num: 662 Reward: 659.9599804865861\n",
      "Total Timesteps: 457024 Episode Num: 663 Reward: 685.3324859378238\n",
      "Total Timesteps: 458024 Episode Num: 664 Reward: 695.730479400519\n",
      "Total Timesteps: 459024 Episode Num: 665 Reward: 857.2792269179334\n",
      "Total Timesteps: 460024 Episode Num: 666 Reward: 759.6715755957626\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 806.605249\n",
      "---------------------------------------\n",
      "Total Timesteps: 461024 Episode Num: 667 Reward: 779.0060069721036\n",
      "Total Timesteps: 462024 Episode Num: 668 Reward: 768.6577386895674\n",
      "Total Timesteps: 463024 Episode Num: 669 Reward: 802.4688110967693\n",
      "Total Timesteps: 464024 Episode Num: 670 Reward: 780.9263283114437\n",
      "Total Timesteps: 465024 Episode Num: 671 Reward: 802.2710919249721\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 703.890473\n",
      "---------------------------------------\n",
      "Total Timesteps: 466024 Episode Num: 672 Reward: 791.7146311644859\n",
      "Total Timesteps: 467024 Episode Num: 673 Reward: 675.7007806754686\n",
      "Total Timesteps: 468024 Episode Num: 674 Reward: 577.5792010840861\n",
      "Total Timesteps: 469024 Episode Num: 675 Reward: 829.8920447915294\n",
      "Total Timesteps: 470024 Episode Num: 676 Reward: 860.2500543392144\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 813.086311\n",
      "---------------------------------------\n",
      "Total Timesteps: 471024 Episode Num: 677 Reward: 733.5600166223968\n",
      "Total Timesteps: 472024 Episode Num: 678 Reward: 869.856943826554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 473024 Episode Num: 679 Reward: 614.285696571191\n",
      "Total Timesteps: 474024 Episode Num: 680 Reward: 864.5545287414958\n",
      "Total Timesteps: 475024 Episode Num: 681 Reward: 765.6397456769067\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 754.224576\n",
      "---------------------------------------\n",
      "Total Timesteps: 476024 Episode Num: 682 Reward: 764.252084124936\n",
      "Total Timesteps: 477024 Episode Num: 683 Reward: 782.8217167669764\n",
      "Total Timesteps: 478024 Episode Num: 684 Reward: 824.57950393297\n",
      "Total Timesteps: 479024 Episode Num: 685 Reward: 870.1834684986726\n",
      "Total Timesteps: 480024 Episode Num: 686 Reward: 783.4279594409782\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 705.083605\n",
      "---------------------------------------\n",
      "Total Timesteps: 481024 Episode Num: 687 Reward: 688.512004051674\n",
      "Total Timesteps: 482024 Episode Num: 688 Reward: 817.759924303774\n",
      "Total Timesteps: 483024 Episode Num: 689 Reward: 755.3622564114137\n",
      "Total Timesteps: 484024 Episode Num: 690 Reward: 780.0937636230674\n",
      "Total Timesteps: 485024 Episode Num: 691 Reward: 776.1814852994556\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 652.907164\n",
      "---------------------------------------\n",
      "Total Timesteps: 486024 Episode Num: 692 Reward: 571.7847517253953\n",
      "Total Timesteps: 487024 Episode Num: 693 Reward: 365.93339298421284\n",
      "Total Timesteps: 488024 Episode Num: 694 Reward: 761.7411332462615\n",
      "Total Timesteps: 489024 Episode Num: 695 Reward: 732.0704247299495\n",
      "Total Timesteps: 490024 Episode Num: 696 Reward: 734.7392698331192\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 776.152818\n",
      "---------------------------------------\n",
      "Total Timesteps: 491024 Episode Num: 697 Reward: 697.6314649490981\n",
      "Total Timesteps: 492024 Episode Num: 698 Reward: 795.6842360844068\n",
      "Total Timesteps: 493024 Episode Num: 699 Reward: 858.5235616221319\n",
      "Total Timesteps: 494024 Episode Num: 700 Reward: 839.7538551731337\n",
      "Total Timesteps: 495024 Episode Num: 701 Reward: 829.6221799330772\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 825.439572\n",
      "---------------------------------------\n",
      "Total Timesteps: 496024 Episode Num: 702 Reward: 730.0236854749277\n",
      "Total Timesteps: 497024 Episode Num: 703 Reward: 760.2592940578645\n",
      "Total Timesteps: 498024 Episode Num: 704 Reward: 395.2960981576107\n",
      "Total Timesteps: 499024 Episode Num: 705 Reward: 935.1465902803468\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 781.291968\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We start the main loop over 500,000 timesteps\n",
    "while total_timesteps < max_timesteps:\n",
    "  \n",
    "  # If the episode is done\n",
    "  if done:\n",
    "\n",
    "    # If we are not at the very beginning, we start the training process of the model\n",
    "    if total_timesteps != 0:\n",
    "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
    "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "\n",
    "    # We evaluate the episode and we save the policy\n",
    "    if timesteps_since_eval >= eval_freq:\n",
    "      timesteps_since_eval %= eval_freq\n",
    "      evaluations.append(evaluate_policy(policy))\n",
    "      policy.save(file_name, directory=\"./pytorch_models\")\n",
    "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "    \n",
    "    # When the training step is done, we reset the state of the environment\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Set the Done to False\n",
    "    done = False\n",
    "    \n",
    "    # Set rewards and episode timesteps to zero\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "  \n",
    "  # Before 10000 timesteps, we play random actions\n",
    "  if total_timesteps < start_timesteps:\n",
    "    action = env.action_space.sample()\n",
    "  else: # After 10000 timesteps, we switch to the model\n",
    "    action = policy.select_action(np.array(obs))\n",
    "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "    if expl_noise != 0:\n",
    "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "  \n",
    "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "  new_obs, reward, done, _ = env.step(action)\n",
    "  \n",
    "  # We check if the episode is done\n",
    "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
    "  \n",
    "  # We increase the total reward\n",
    "  episode_reward += reward\n",
    "  \n",
    "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
    "\n",
    "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
    "  obs = new_obs\n",
    "  episode_timesteps += 1\n",
    "  total_timesteps += 1\n",
    "  timesteps_since_eval += 1\n",
    "\n",
    "# We add the last policy evaluation to our list of evaluations and we save our model\n",
    "evaluations.append(evaluate_policy(policy))\n",
    "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
    "np.save(\"./results/%s\" % (file_name), evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi6e2-_pu05e"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oW4d1YAMqif1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_AntBulletEnv-v0_0\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 737.611418\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer_1 = nn.Linear(state_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
    "    return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, 1)\n",
    "    # Defining the second Critic neural network\n",
    "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_5 = nn.Linear(400, 300)\n",
    "    self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "  def forward(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Forward-Propagation on the second Critic Neural Network\n",
    "    x2 = F.relu(self.layer_4(xu))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    return x1\n",
    "\n",
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "    return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "    for it in range(iterations):\n",
    "      \n",
    "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "      next_action = self.actor_target(next_state)\n",
    "      \n",
    "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "      \n",
    "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "      \n",
    "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "      \n",
    "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "      \n",
    "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "      \n",
    "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "      if it % policy_freq == 0:\n",
    "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "  # Making a save method to save a trained model\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "\n",
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(np.array(obs))\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward\n",
    "\n",
    "env_name = \"AntBulletEnv-v0\"\n",
    "seed = 0\n",
    "\n",
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")\n",
    "\n",
    "eval_episodes = 10\n",
    "save_env_vid = True\n",
    "env = gym.make(env_name)\n",
    "max_episode_steps = env._max_episode_steps\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "policy = TD3(state_dim, action_dim, max_action)\n",
    "policy.load(file_name, './pytorch_models/')\n",
    "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TD3_Ant.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "fdd41deda711749ed0eecb680569f0f47ed21afdd6d3be9f7d0f767172fcd296"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
